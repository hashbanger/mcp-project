{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\", \n",
    "    instructions=\"You are a helpful assistant that can answer questions and help with tasks.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a quaint little village nestled between two hills, there lived a curious girl named Lila. Every day after school, she explored the nearby woods, making friends with the animals and gathering wildflowers.\\n\\nOne sunny afternoon, Lila stumbled upon a hidden glade filled with vibrant flowers and the soft hum of bees. At the center stood a majestic tree, its trunk thick and gnarled, with branches reaching high into the sky. Curious, she approached and noticed a small door carved into the bark.\\n\\nWith a gentle push, the door creaked open, revealing a shimmering light inside. Hesitant but excited, Lila stepped through the doorway and found herself in a magical world filled with talking animals and twinkling stars overhead.\\n\\nA wise old owl welcomed her and explained that this was a realm where dreams came to life. Lila shared her wish to understand the language of the animals in her village. The owl nodded and granted her wish, but with one condition: she must always listen and respect them.\\n\\nBack in her village, Lila could now hear the whispers of the birds and the tales of the rabbits. She became a bridge between the humans and the animals, helping them understand each other.\\n\\nAnd from that day on, the village thrived, filled with harmony, laughter, and the beauty of friendship. Lilaâ€™s heart swelled with joy, knowing she had made the world a little brighter for everyone around her.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(starting_agent=agent, input=\"tell me a short story\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once, in a quaint village surrounded by whispering woods, lived an elderly woman named Esme. She was known for her magical garden filled with flowers that shimmered under the moonlight. Every evening, children gathered at her gate, eager to hear her tales of adventure and wonder.\n",
      "\n",
      "One night, a curious boy named Leo approached her. \"Esme, do you have a story about stars?\" he asked, eyes wide with anticipation. \n",
      "\n",
      "With a gentle smile, Esme began, \"Long ago, the stars fell from the sky, scattering across the earth. The world grew dim without their light, and the creatures of the night grew sad. A brave little firefly named Luma decided to embark on a journey to gather the stars and return them home.\"\n",
      "\n",
      "As Esme spoke, the village around them transformed. The air was filled with shimmering lights as if Lumaâ€™s journey was unfolding before their eyes. They envisioned Luma flying through dark forests, navigating rivers, and facing fierce winds, all the while collecting the fallen stars, each one more beautiful than the last.\n",
      "\n",
      "Eventually, with perseverance and heart, Luma reached the highest mountain where all lost stars gathered. With a mighty glow, she encouraged them to rise back into the sky. One by one, the stars lifted off the ground, twinkling brightly as they ascended.\n",
      "\n",
      "\"And from that night on,\" Esme concluded, \"whenever you see a shooting star, remember itâ€™s Lumaâ€™s spirit, reminding us that even the smallest among us can make the biggest difference.\"\n",
      "\n",
      "The children cheered, and as they turned to leave, the sky above sparkled with constellations, a magical reminder of Esmeâ€™s story. From that night onward, whenever they looked up, they saw not just stars, but a brave little firefly chasing dreams."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\"\n",
    ")\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two floating numbers and return the result.\"\"\"\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to answer queries.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[multiply],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"x\":7.814,\"y\":103.892}\n",
      "> Tool Called, name: multiply\n",
      "> Tool Called, args: {\"x\":7.814,\"y\":103.892}\n",
      "> Tool Output: 811.812088\n",
      "The result of \\( 7.814 \\times 103.892 \\) is approximately \\( 811.812 \\)."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import (\n",
    "    ResponseFunctionCallArgumentsDeltaEvent,\n",
    "    # ResponseCreatedEvent\n",
    ")\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"What is 7.814 times 103.892?\"\n",
    ")\n",
    "\n",
    "# async for event in response.stream_events():\n",
    "#     print(event)\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\":\n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True) # this is streamed parameters for our tool call\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True) # this is streamed final answer tokens\n",
    "    \n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # this tells us which agent is currently in use\n",
    "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "    \n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # this contains the information that we'd typically stream out to the user\n",
    "        if event.name == \"tool_called\":\n",
    "            # this is the collection of oru _full_ tool call after our tool tokens have all been streamed\n",
    "            print()\n",
    "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # this is the response from our tool execution\n",
    "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class GuardrailOutput(BaseModel):\n",
    "    is_triggered: bool\n",
    "    reasoning: str\n",
    "\n",
    "politics_agent = Agent(\n",
    "    name=\"Politics Agent\",\n",
    "    instructions=\"Check if user is asking you about political opinions.\",\n",
    "    output_type=GuardrailOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GuardrailOutput(is_triggered=True, reasoning=\"The user's question directly asks for an opinion about the Labor Party in the UK, which is a political opinion request.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What do you think about the labor party in the UK?\"\n",
    "\n",
    "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import (\n",
    "    GuardrailFunctionOutput,\n",
    "    RunContextWrapper,\n",
    "    input_guardrail\n",
    ")\n",
    "\n",
    "@input_guardrail\n",
    "async def politics_guardrail(\n",
    "    context: RunContextWrapper[None],\n",
    "    agent: Agent,\n",
    "    input: str\n",
    ") -> GuardrailFunctionOutput:\n",
    "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=response.final_output,\n",
    "        tripwire_triggered=response.final_output.is_triggered\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to answer queries.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[multiply],\n",
    "    input_guardrails=[politics_guardrail]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InputGuardrailTripwireTriggered",
     "evalue": "Guardrail InputGuardrail triggered tripwire",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m      2\u001b[0m     starting_agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do you think about the labor party in the UK?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m response\u001b[38;5;241m.\u001b[39mfinal_output\n",
      "File \u001b[0;32m~/prashant_b/.venv/lib/python3.10/site-packages/agents/run.py:210\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[1;32m    205\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[1;32m    212\u001b[0m             starting_agent,\n\u001b[1;32m    213\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[1;32m    214\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[1;32m    215\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[1;32m    216\u001b[0m             context_wrapper,\n\u001b[1;32m    217\u001b[0m         ),\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    219\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    220\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m    221\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[1;32m    222\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    223\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[1;32m    224\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    225\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[1;32m    226\u001b[0m         ),\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    230\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    231\u001b[0m         original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m         should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[1;32m    237\u001b[0m     )\n",
      "File \u001b[0;32m~/prashant_b/.venv/lib/python3.10/site-packages/agents/run.py:805\u001b[0m, in \u001b[0;36mRunner._run_input_guardrails\u001b[0;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[1;32m    798\u001b[0m         t\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    799\u001b[0m     _error_tracing\u001b[38;5;241m.\u001b[39mattach_error_to_current_span(\n\u001b[1;32m    800\u001b[0m         SpanError(\n\u001b[1;32m    801\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail tripwire triggered\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    802\u001b[0m             data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguardrail\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mguardrail\u001b[38;5;241m.\u001b[39mget_name()},\n\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    804\u001b[0m     )\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InputGuardrailTripwireTriggered(result)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     guardrail_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m: Guardrail InputGuardrail triggered tripwire"
     ]
    }
   ],
   "source": [
    "response = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"What do you think about the labor party in the UK?\"\n",
    ")\n",
    "response.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversational Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add two floating numbers and return the result.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@function_tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract two floating numbers and return the result.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two floating numbers and return the result.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@function_tool\n",
    "def divide(x: float, y: float) -> float:\n",
    "    \"\"\"Divide two floating numbers and return the result.\"\"\"\n",
    "    return x / y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to answer queries.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[add, subtract, multiply, divide],\n",
    "    input_guardrails=[politics_guardrail]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \n",
      "Assistant: It seems like your message didn't come through. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation\n",
    "user_input = \"start conversation\"\n",
    "conversation_history = []\n",
    "\n",
    "# Main conversation loop\n",
    "while user_input and user_input.lower() not in [\"quit\", \"exit\", \"q\", \"bye\"]:\n",
    "    # Display and get user input\n",
    "    user_input = input()\n",
    "    print(f\"User: {user_input}\")\n",
    "    \n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_input\n",
    "    })\n",
    "\n",
    "    # Get AI response\n",
    "    response = await Runner.run(\n",
    "        starting_agent=agent,\n",
    "        input=conversation_history\n",
    "    )\n",
    "    \n",
    "    # Display AI response and update history\n",
    "    print(f\"Assistant: {response.final_output}\")\n",
    "    conversation_history = response.to_input_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "> Current Agent: Assistant\n",
      "Hello! How can I assist you today?User: what can you do?\n",
      "> Current Agent: Assistant\n",
      "I can help with a variety of tasks, including:\n",
      "\n",
      "1. Performing basic math operations (addition, subtraction, multiplication, division).\n",
      "2. Answering questions or providing information on a wide range of topics.\n",
      "3. Assisting with problem-solving or providing explanations.\n",
      "\n",
      "What would you like help with today?User: I need to multiple 0.22 with 1.54\n",
      "> Current Agent: Assistant\n",
      "{\"x\":0.22,\"y\":1.54}\n",
      "> Tool Called, name: multiply\n",
      "> Tool Called, args: {\"x\":0.22,\"y\":1.54}\n",
      "> Tool Output: 0.3388\n",
      "The result of multiplying 0.22 by 1.54 is 0.3388.User: Thanks a ton for that\n",
      "> Current Agent: Assistant\n",
      "You're welcome! If you have any more questions or need further assistance, feel free to ask!User: okay\n",
      "> Current Agent: Assistant\n",
      "Great! Just let me know if there's anything specific you need help with.User: bye\n",
      "> Current Agent: Assistant\n",
      "Goodbye! Have a great day! If you need assistance in the future, feel free to reach out."
     ]
    }
   ],
   "source": [
    "# Initialize conversation\n",
    "user_input = \"start conversation\"\n",
    "conversation_history = []\n",
    "\n",
    "# Main conversation loop\n",
    "while user_input and user_input.lower() not in [\"quit\", \"exit\", \"q\", \"bye\"]:\n",
    "    # Display and get user input\n",
    "    user_input = input()\n",
    "    print(f\"User: {user_input}\")\n",
    "    \n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_input\n",
    "    })\n",
    "\n",
    "    # response = await Runner.run(\n",
    "    #     starting_agent=agent,\n",
    "    #     input=conversation_history\n",
    "    # )\n",
    "    response = Runner.run_streamed(\n",
    "        starting_agent=agent,\n",
    "        input=conversation_history\n",
    "    )\n",
    "\n",
    "    async for event in response.stream_events():\n",
    "        if event.type == \"raw_response_event\":\n",
    "            if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "                print(event.data.delta, end=\"\", flush=True) # this is streamed parameters for our tool call\n",
    "            elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                print(event.data.delta, end=\"\", flush=True) # this is streamed final answer tokens\n",
    "        \n",
    "        elif event.type == \"agent_updated_stream_event\":\n",
    "            # this tells us which agent is currently in use\n",
    "            print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "        \n",
    "        elif event.type == \"run_item_stream_event\":\n",
    "            # this contains the information that we'd typically stream out to the user\n",
    "            if event.name == \"tool_called\":\n",
    "                # this is the collection of oru _full_ tool call after our tool tokens have all been streamed\n",
    "                print()\n",
    "                print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "                print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "            elif event.name == \"tool_output\":\n",
    "                # this is the response from our tool execution\n",
    "                print(f\"> Tool Output: {event.item.raw_item['output']}\")\n",
    "\n",
    "    # Display AI response and update history\n",
    "    # print(f\"Assistant: {response.final_output}\")\n",
    "    conversation_history = response.to_input_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handoff Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add two floating numbers and return the result.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@function_tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract two floating numbers and return the result.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two floating numbers and return the result.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@function_tool\n",
    "def divide(x: float, y: float) -> float:\n",
    "    \"\"\"Divide two floating numbers and return the result.\"\"\"\n",
    "    return x / y\n",
    "\n",
    "\n",
    "calculator_agent = Agent(\n",
    "    name=\"Calculator Assistant\",\n",
    "    instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to answer queries.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[add, subtract, multiply, divide]\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather temperature for a city. Always returns 35Â°C.\"\"\"\n",
    "    return f\"The current temperature in {city} is 35Â°C\"\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Assistant\",\n",
    "    instructions=\"You're a helpful weather assistant. Always use the provided weather tool to check temperatures. Do not make up weather information and rely on the tool's data. Always give the temperature in Farhrenheit.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_weather]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "main_agent = Agent(\n",
    "    name=\"Main Agent\",\n",
    "    instructions=(\n",
    "        \"You are minerva, a bot by BCG POPX. You are always cheerful and happy to help.\"\n",
    "        \"Help the user with their questions.\"\n",
    "        \"If they ask about getather, handoff to the booking agent.\"\n",
    "        \"If they ask about calculations, handoff to the calculator agent.\"\n",
    "    ),\n",
    "    handoffs=[calculator_agent, weather_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in California is 35Â°C, which is approximately 95Â°F. If you need more specific weather information or forecasts, let me know!\n"
     ]
    }
   ],
   "source": [
    "from agents import Runner\n",
    "\n",
    "response = await Runner.run(\n",
    "    starting_agent=main_agent,\n",
    "    input=response.to_input_list() + [{\"role\": \"user\", \"content\": \"What's the weather in california!\"}]\n",
    ")\n",
    "print(response.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'exit' to quit):\n",
      "\n",
      "Bot: Hi Prashant! Howâ€™s your day going?\n",
      "Bot: I can chat with you, answer questions, help with information, and offer recommendations. Anything specific you need help with?\n",
      "Bot: Of course, Prashant! What would you like to talk about today?\n",
      "Bot: You're welcome! If there's anything else you need, just let me know.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from agents import Agent, Runner, RunContextWrapper\n",
    "\n",
    "@dataclass\n",
    "class ChatContext:\n",
    "    user_name: str\n",
    "    history: list = field(default_factory=list)\n",
    "\n",
    "# Set up the agent\n",
    "agent = Agent[ChatContext](\n",
    "    name=\"ChatBot\",\n",
    "    instructions=\"Be concise and friendly. Refer to previous messages if helpful.\",\n",
    ")\n",
    "\n",
    "async def handle_message(user_message: str, context: ChatContext):\n",
    "    context.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    result = await Runner.run(\n",
    "        starting_agent=agent,\n",
    "        input=context.history,  # Feed full history to LLM\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    context.history.append({\"role\": \"assistant\", \"content\": result.final_output})\n",
    "    return result.final_output\n",
    "\n",
    "async def main():\n",
    "    context = ChatContext(user_name=\"Alex\")\n",
    "\n",
    "    print(\"Start chatting with the bot (type 'exit' to quit):\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = await handle_message(user_input, context)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# For Jupyter/Notebook\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Minerva ðŸ¤– (type 'exit' to quit)\n",
      "\n",
      "Minerva: RunResult:\n",
      "- Last agent: Agent(name=\"Main Agent\", ...)\n",
      "- Final output (str):\n",
      "    It looks like your message got cut off! How can I assist you today? ðŸ˜Š\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "Minerva: RunResult:\n",
      "- Last agent: Agent(name=\"Main Agent\", ...)\n",
      "- Final output (str):\n",
      "    It seems like you're trying to ask something. Could you please provide a bit more detail? I'm here to help! ðŸŽ‰\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "Minerva: Goodbye! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from agents import Agent, Runner, RunContextWrapper, function_tool\n",
    "\n",
    "# === TOOLS ===\n",
    "\n",
    "@function_tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    return x + y\n",
    "\n",
    "@function_tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    return x - y\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    return x * y\n",
    "\n",
    "@function_tool\n",
    "def divide(x: float, y: float) -> float:\n",
    "    return x / y\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The current temperature in {city} is 35Â°C\"\n",
    "\n",
    "# === SPECIALIZED AGENTS ===\n",
    "\n",
    "calculator_agent = Agent(\n",
    "    name=\"Calculator Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful assistant, remember to always use the provided tools whenever possible. \"\n",
    "        \"Do not rely on your own knowledge too much and instead use your tools to answer queries.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[add, subtract, multiply, divide]\n",
    ")\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful weather assistant. Always use the provided weather tool to check temperatures. \"\n",
    "        \"Do not make up weather information and rely on the tool's data. Always give the temperature in Fahrenheit.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_weather]\n",
    ")\n",
    "\n",
    "# === CONTEXT ===\n",
    "\n",
    "@dataclass\n",
    "class ChatContext:\n",
    "    user_name: str\n",
    "    history: list = field(default_factory=list)\n",
    "\n",
    "# === MAIN AGENT ===\n",
    "\n",
    "\n",
    "main_agent = Agent[ChatContext](\n",
    "    name=\"Main Agent\",\n",
    "    instructions=(\n",
    "        \"You are Minerva, a bot by BCG POPX. You are always cheerful and happy to help. \"\n",
    "        \"Help the user with their questions. \"\n",
    "        \"If they ask about weather, hand off to the Weather Assistant. \"\n",
    "        \"If they ask about calculations, hand off to the Calculator Assistant.\"\n",
    "    ),\n",
    "    handoffs=[calculator_agent, weather_agent],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# === MESSAGE HANDLER ===\n",
    "\n",
    "async def handle_message(user_message: str, context: ChatContext):\n",
    "    context.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    result = await Runner.run(\n",
    "        starting_agent=main_agent,\n",
    "        input=context.history,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    context.history.append({\"role\": \"assistant\", \"content\": result.final_output})\n",
    "    return result\n",
    "\n",
    "# === CHAT LOOP ===\n",
    "\n",
    "async def main():\n",
    "    context = ChatContext(user_name=\"Alex\")\n",
    "    print(\"Welcome to Minerva ðŸ¤– (type 'exit' to quit)\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.strip().lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Minerva: Goodbye! ðŸ‘‹\")\n",
    "            break\n",
    "\n",
    "        response = await handle_message(user_input, context)\n",
    "        print(\"Minerva:\", response)\n",
    "\n",
    "# === RUN (for Jupyter/IPython) ===\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ChatContext(user_name=\"Alex\")\n",
    "resp = await handle_message(\"What's the weather in San Francisco?\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Handoff occurred, but target agent not found.\n",
      "[LOG] Agent 'Weather Assistant' is invoking tool 'get_weather' with arguments: {\"city\":\"San Francisco\"}\n",
      "[LOG] Agent 'Weather Assistant' received output: The current temperature in San Francisco is 35Â°C from tool call\n",
      "[LOG] Agent 'Weather Assistant' responded with: \"The current temperature in San Francisco is 95Â°F.\"\n"
     ]
    }
   ],
   "source": [
    "log_current_tool_execution(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_current_tool_execution(run_result):\n",
    "    for item in run_result.new_items:\n",
    "        # Case 1: Handoff occurred\n",
    "        if item.type == \"handoff_call_item\":\n",
    "            # Get the agent name (if available)\n",
    "            if hasattr(item, \"target_agent\"):\n",
    "                print(f\"[LOG] Handoff to agent: {item.target_agent.name}\")\n",
    "            else:\n",
    "                print(\"[LOG] Handoff occurred, but target agent not found.\")\n",
    "\n",
    "        # Case 2: Tool is being called\n",
    "        elif item.type == \"tool_call_item\":\n",
    "            tool_name = getattr(item.raw_item, \"name\", \"unknown_tool\")\n",
    "            args = getattr(item.raw_item, \"arguments\", \"{}\")\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[LOG] Agent '{agent_name}' is invoking tool '{tool_name}' with arguments: {args}\")\n",
    "\n",
    "        # Case 3: Tool execution output\n",
    "        elif item.type == \"tool_call_output_item\":\n",
    "            output = getattr(item, \"output\", None)\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[LOG] Agent '{agent_name}' received output: {output} from tool call\")\n",
    "\n",
    "        # Case 4: Assistant final message\n",
    "        elif item.type == \"message_output_item\":\n",
    "            content = (\n",
    "                item.raw_item.content[0].text\n",
    "                if getattr(item.raw_item, \"content\", None)\n",
    "                else \"[no message content]\"\n",
    "            )\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[LOG] Agent '{agent_name}' responded with: \\\"{content}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunResult' object has no attribute 'stream_events'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream_events():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_response_event\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata, ResponseFunctionCallArgumentsDeltaEvent):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RunResult' object has no attribute 'stream_events'"
     ]
    }
   ],
   "source": [
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\":\n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True) # this is streamed parameters for our tool call\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True) # this is streamed final answer tokens\n",
    "    \n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # this tells us which agent is currently in use\n",
    "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "    \n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # this contains the information that we'd typically stream out to the user\n",
    "        if event.name == \"tool_called\":\n",
    "            # this is the collection of oru _full_ tool call after our tool tokens have all been streamed\n",
    "            print()\n",
    "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # this is the response from our tool execution\n",
    "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_current_tool_execution(result):\n",
    "    \"\"\"Log tool execution details using Python's logging module.\"\"\"\n",
    "    for item in result.new_items:\n",
    "        # Case 1: Handoff occurred\n",
    "        if item.type == \"handoff_call_item\":\n",
    "            # Get the agent name (if available)\n",
    "            if hasattr(item, \"target_agent\"):\n",
    "                print(f\"Handoff to agent: {item.target_agent.name}\")\n",
    "            else:\n",
    "                print(\"[WARNING] Handoff occurred, but target agent not found.\")\n",
    "\n",
    "        # Case 2: Tool is being called\n",
    "        elif item.type == \"tool_call_item\":\n",
    "            tool_name = getattr(item.raw_item, \"name\", \"unknown_tool\")\n",
    "            args = getattr(item.raw_item, \"arguments\", \"{}\")\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[INFO] Agent '{agent_name}' is invoking tool '{tool_name}' with arguments: {args}\")\n",
    "\n",
    "        # Case 3: Tool execution output\n",
    "        elif item.type == \"tool_call_output_item\":\n",
    "            output = getattr(item, \"output\", None)\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[INFO] Agent '{agent_name}' received output: {output} from tool call\")\n",
    "\n",
    "        # Case 4: Assistant final message\n",
    "        elif item.type == \"message_output_item\":\n",
    "            content = (\n",
    "                item.raw_item.content[0].text\n",
    "                if getattr(item.raw_item, \"content\", None)\n",
    "                else \"[no message content]\"\n",
    "            )\n",
    "            agent_name = getattr(item.agent, \"name\", \"unknown_agent\")\n",
    "            print(f\"[INFO] Agent '{agent_name}' responded with: \\\"{content}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# Imports\n",
    "# =================\n",
    "from dataclasses import dataclass, field\n",
    "from agents import Agent, Runner, function_tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "# =================\n",
    "# Tool Functions\n",
    "# =================\n",
    "@function_tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    print(\"Adding\", x, y)\n",
    "    return x + y\n",
    "\n",
    "@function_tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    print(\"Subtracting\", x, y)\n",
    "    return x - y\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    print(\"Multiplying\", x, y)\n",
    "    return x * y\n",
    "\n",
    "@function_tool\n",
    "def divide(x: float, y: float) -> float:\n",
    "    print(\"Dividing\", x, y)\n",
    "    return x / y\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The current temperature in {city} is 35Â°C\"\n",
    "\n",
    "# =================\n",
    "# Response Structure\n",
    "# =================\n",
    "class ResponseStucture(BaseModel):\n",
    "    message: str\n",
    "    message_type: str # 'text' or 'checkbox'\n",
    "    options: list[str]\n",
    "    trigger: bool # always keep it false\n",
    "\n",
    "# =================\n",
    "# Agent Definitions\n",
    "# =================\n",
    "calculator_agent = Agent(\n",
    "    name=\"Calculator Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful assistant, remember to always use the provided tools whenever possible. \"\n",
    "        \"Do not rely on your own knowledge too much and instead use your tools to answer queries.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[add, subtract, multiply, divide],\n",
    ")\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Assistant\",\n",
    "    instructions=(\n",
    "        \"You're a helpful weather assistant. Always use the provided weather tool to check temperatures. \"\n",
    "        \"Do not make up weather information and rely on the tool's data.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "# =================\n",
    "# Data Models\n",
    "# =================\n",
    "@dataclass\n",
    "class ChatContext:\n",
    "    user_name: str\n",
    "    history: list = field(default_factory=list)\n",
    "\n",
    "# =================\n",
    "# Main Agent Setup\n",
    "# =================\n",
    "\n",
    "main_agent = Agent[ChatContext](\n",
    "    name=\"Main Agent\",\n",
    "    instructions=(\n",
    "        \"You are Novak, a bot by BCG POPX. You are always cheerful and happy to help. \"\n",
    "        \"Help the user with their questions.\"\n",
    "        \"You have access to two tools: Calculator and Weather.\"\n",
    "        # \"If they ask about weather calculations, hand off to the Weather Assistant first then handoff that result to Calculator Assistant.\"\n",
    "        \"Whenever you don't understand the user's question, ask questions back to understand the user's question better. And try to break it down to solve it.\"\n",
    "        \"Try to solve the problem with combination of tools available. Check if result of one tool can be used as input for another tool.\"\n",
    "\n",
    "        \"Response Stucture: You can give output in either only text or checkbox (If you want user to select from options) format. message_type must be 'text' or 'checkbox'. Use 'options' array to specify the options for the checkbox.\"\n",
    "    ),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ResponseStucture,\n",
    "    tools=[\n",
    "            calculator_agent.as_tool(tool_name=\"calculator\", tool_description=\"Tool to perform calculations\"), \n",
    "            weather_agent.as_tool(tool_name=\"weather\", tool_description=\"Tool to get weather information\")\n",
    "        ]\n",
    "    # handoffs=[calculator_agent]\n",
    ")\n",
    "\n",
    "# =================\n",
    "# Message Handler\n",
    "# =================\n",
    "async def handle_message(user_message: str, context: ChatContext):\n",
    "    context.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    result = await Runner.run(\n",
    "        starting_agent=main_agent,\n",
    "        input=context.history,\n",
    "        context=context\n",
    "    )\n",
    "    log_current_tool_execution(result)\n",
    "    context.history.append({\"role\": \"assistant\", \"content\": str(result.final_output)})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplying 35.0 0.00234\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'weather' with arguments: {\"input\":\"San Francisco\"}\n",
      "[INFO] Agent 'Main Agent' received output: The current temperature in San Francisco is 35Â°C. Is there anything else you would like to know? from tool call\n",
      "[WARNING] Handoff occurred, but target agent not found.\n",
      "[INFO] Agent 'Calculator Assistant' is invoking tool 'multiply' with arguments: {\"x\":35,\"y\":0.00234}\n",
      "[INFO] Agent 'Calculator Assistant' received output: 0.0819 from tool call\n",
      "[INFO] Agent 'Calculator Assistant' responded with: \"The result of multiplying the temperature of San Francisco (35Â°C) by 0.00234 is approximately **0.0819**.\"\n",
      "The result of multiplying the temperature of San Francisco (35Â°C) by 0.00234 is approximately **0.0819**.\n"
     ]
    }
   ],
   "source": [
    "context = ChatContext(user_name=\"Testing\")\n",
    "\n",
    "response = await handle_message(\"I need to do a weather calculation. I need to multiply the temperature of San Francisco by 0.00234\", context)\n",
    "# dict(response.final_output)\n",
    "print(response.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'hello there I am prashant'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"message='Hello Prashant! ðŸ˜Š How can I assist you today?' message_type='text' options=[] trigger=False\"},\n",
       " {'role': 'user', 'content': 'What is your name?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"message='Iâ€™m Novak, your cheerful assistant! How can I help you today?' message_type='text' options=[] trigger=False\"},\n",
       " {'role': 'user', 'content': 'I need to solve a problem'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"message='What problem would you like to solve? If you can share some details, Iâ€™d be happy to help!' message_type='text' options=[] trigger=False\"},\n",
       " {'role': 'user', 'content': 'Can you help me?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"message='I can definitely help you! Here are two things I found:\\\\n1. The result of 2 + 2 is **4**.\\\\n2. The current temperature in New York is **35Â°C**. \\\\n\\\\nWhat else do you need assistance with?' message_type='text' options=[] trigger=False\"},\n",
       " {'role': 'user', 'content': 'quit'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"message='If you have any more questions or need assistance in the future, feel free to ask. Have a great day!' message_type='text' options=[] trigger=False\"}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent 'Main Agent' responded with: \"{\"message\":\"Hello Prashant! ðŸ˜Š How can I assist you today?\",\"message_type\":\"text\",\"options\":[],\"trigger\":false}\"\n",
      "{'message': 'Hello Prashant! ðŸ˜Š How can I assist you today?', 'message_type': 'text', 'options': [], 'trigger': False}\n",
      "[INFO] Agent 'Main Agent' responded with: \"{\"message\":\"Iâ€™m Novak, your cheerful assistant! How can I help you today?\",\"message_type\":\"text\",\"options\":[],\"trigger\":false}\"\n",
      "{'message': 'Iâ€™m Novak, your cheerful assistant! How can I help you today?', 'message_type': 'text', 'options': [], 'trigger': False}\n",
      "Adding 1.0 1.0\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'calculator' with arguments: {\"input\":\"1 + 1\"}\n",
      "[INFO] Agent 'Main Agent' received output: The result of \\( 1 + 1 \\) is \\( 2 \\). from tool call\n",
      "[INFO] Agent 'Main Agent' responded with: \"{\"message\":\"What problem would you like to solve? If you can share some details, Iâ€™d be happy to help!\",\"message_type\":\"text\",\"options\":[],\"trigger\":false}\"\n",
      "{'message': 'What problem would you like to solve? If you can share some details, Iâ€™d be happy to help!', 'message_type': 'text', 'options': [], 'trigger': False}\n",
      "Adding 2.0 2.0\n",
      "Multiplying 5.0 5.0\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'calculator' with arguments: {\"input\":\"2+2\"}\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'weather' with arguments: {\"input\":\"current weather\"}\n",
      "[INFO] Agent 'Main Agent' received output: The result of \\( 2 + 2 \\) is \\( 4 \\). from tool call\n",
      "[INFO] Agent 'Main Agent' received output: Could you please specify the city for which you would like to know the current weather? from tool call\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'calculator' with arguments: {\"input\":\"5*5\"}\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'weather' with arguments: {\"input\":\"New York\"}\n",
      "[INFO] Agent 'Main Agent' received output: The result of \\( 5 \\times 5 \\) is \\( 25 \\). from tool call\n",
      "[INFO] Agent 'Main Agent' received output: The current temperature in New York is 35Â°C. from tool call\n",
      "[INFO] Agent 'Main Agent' responded with: \"{\"message\":\"I can definitely help you! Here are two things I found:\\n1. The result of 2 + 2 is **4**.\\n2. The current temperature in New York is **35Â°C**. \\n\\nWhat else do you need assistance with?\",\"message_type\":\"text\",\"options\":[],\"trigger\":false}\"\n",
      "{'message': 'I can definitely help you! Here are two things I found:\\n1. The result of 2 + 2 is **4**.\\n2. The current temperature in New York is **35Â°C**. \\n\\nWhat else do you need assistance with?', 'message_type': 'text', 'options': [], 'trigger': False}\n",
      "[INFO] Agent 'Main Agent' is invoking tool 'calculator' with arguments: {\"input\":\"quit\"}\n",
      "[INFO] Agent 'Main Agent' received output: If you have any more questions or need assistance in the future, feel free to ask. Have a great day! from tool call\n",
      "[INFO] Agent 'Main Agent' responded with: \"{\"message\":\"If you have any more questions or need assistance in the future, feel free to ask. Have a great day!\",\"message_type\":\"text\",\"options\":[],\"trigger\":false}\"\n",
      "{'message': 'If you have any more questions or need assistance in the future, feel free to ask. Have a great day!', 'message_type': 'text', 'options': [], 'trigger': False}\n"
     ]
    }
   ],
   "source": [
    "context = ChatContext(user_name=\"Prashant\")\n",
    "\n",
    "user_message = None\n",
    "while user_message not in [\"exit\", \"quit\"]:\n",
    "    user_message = input(\"Enter your message: \")\n",
    "    response = await handle_message(user_message, context)\n",
    "    print(dict(response.final_output))\n",
    "    # print(response.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"matches\": List[str]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import GPT\n",
    "\n",
    "llm = GPT()\n",
    "import json\n",
    "\n",
    "response = llm.get_response(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me an mcq on machine learning\"}\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"essay\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"message\": {\"type\": \"string\"},\n",
    "                    \"message_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"text\", \"checkbox\", \"radio\", \"dropdown\"]\n",
    "                    },\n",
    "                    \"options\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"message\", \"message_type\", \"options\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response_json = json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class GPT:\n",
    "    \"\"\"\n",
    "    A wrapper class for interacting with OpenAI's GPT models.\n",
    "\n",
    "    Attributes:\n",
    "        api_key (str): The API key for authentication with OpenAI.\n",
    "        organization_key (str, optional): The organization ID for OpenAI.\n",
    "        client (OpenAI): The OpenAI client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = None,\n",
    "        organization_key: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GPT class with OpenAI credentials.\n",
    "        Args:\n",
    "            api_key (str, optional): OpenAI API key. Defaults to None, in which case\n",
    "                it is retrieved from environment variables.\n",
    "            organization_key (str, optional): OpenAI organization ID. Defaults to None,\n",
    "                retrieved from environment variables.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no API key is provided.\n",
    "        \"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", api_key)\n",
    "        organization_key = os.getenv(\"OPENAI_ORGANIZATION_ID\", organization_key)\n",
    "\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"An API key must be provided either as an argument or in the \"\n",
    "                \"environment variable 'OPENAI_API_KEY'. This is the recommended approach.\"\n",
    "            )\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.organization_key = organization_key\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            organization=self.organization_key\n",
    "        )\n",
    "\n",
    "    def get_response(self, **kwargs: Dict[str, Any]) -> Any:\n",
    "        \"\"\"Sends a request to the OpenAI API and returns the response.\"\"\"\n",
    "        response = self.client.responses.create(**kwargs)\n",
    "        return response\n",
    "\n",
    "\n",
    "llm = GPT()\n",
    "\n",
    "def get_findings(data: pd.DataFrame, client_name: str, peer_list: List[str], analysis_type: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Get insights and findings from data analysis.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data to analyze\n",
    "        client_name (str): Name of the client company\n",
    "        peer_list (List[str]): List of peer companies to compare against\n",
    "        analysis_type (str): Type of analysis to perform\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary containing findings from the analysis\n",
    "    \"\"\"\n",
    "    findings_prompt = \"\"\"I will provide you with a pandas DataFrame and exactly one ANALYSIS_TYPE from the list below. You'll also be given the CLIENT company name and a list of PEERS.\n",
    "Analysis types (one-liner):\n",
    "Manager Title Ratio: % of managers in each company\n",
    "Support Function Analysis: % and breakdown of support functions\n",
    "Compensation Distribution: compensation medians or ranges across levels\n",
    "Interview Experience: candidate interview-experience scores (higher = better)\n",
    "Diversity in Management: % female managers\n",
    "Culture & Values Rating: Glassdoor rating for culture & values\n",
    "\n",
    "Your task:\n",
    "Identify 2â€“3 key insights, focusing on how CLIENT compares to its PEERS in the given analysis.\n",
    "Use specific metrics or percentages to illustrate each point.\n",
    "Write your findings in 2â€“3 concise sentences.\n",
    "\n",
    "Placeholders:\n",
    "DATAFRAME: {input_data} (e.g., df)\n",
    "ANALYSIS_TYPE: {analysis_type} one of the six types above (only one per prompt)\n",
    "CLIENT: {client_name} the focal company\n",
    "PEERS: {peers_list} a list of peer companies ( if not provided refer to the data frame )\"\"\"\n",
    "    \n",
    "    response = llm.get_response(\n",
    "        # model=\"gpt-4o-mini-2024-07-18\",\n",
    "        # model=\"gpt-4.1-nano\",\n",
    "        # model=\"gpt-4.1-nano\",\n",
    "        model=\"o4-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"user\", \"content\": findings_prompt.format(\n",
    "                input_data=data,\n",
    "                analysis_type=analysis_type,\n",
    "                client_name=client_name,\n",
    "                peers_list=peer_list\n",
    "            )}\n",
    "        ],\n",
    "        text={\n",
    "            \"format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"name\": \"findings\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"findings\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A single paragraph of findings.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"findings\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return json.loads(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\n",
    "    \"Company\": [\n",
    "        \"Archer Daniels Midland\",\n",
    "        \"Nike\",\n",
    "        \"PepsiCo\",\n",
    "        \"Procter & Gamble\",\n",
    "        \"Tyson Foods\"\n",
    "    ],\n",
    "    \"CXO\": [0.1, 0.1, 0.4, 2.2, 0.0],\n",
    "    \"Vice President\": [1.1, 0.2, 0.6, 0.8, 1.1],\n",
    "    \"Director\": [7.9, 7.3, 8.9, 12.4, 14.6],\n",
    "    \"Manager\": [29.7, 26.0, 37.9, 41.5, 41.6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Î¼s, sys: 0 ns, total: 2 Î¼s\n",
      "Wall time: 5.25 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "findings = get_findings(data=sample_data, client_name=\"PepsiCo\", peer_list=[\"Archer Daniels Midland\", \"Nike\", \"Procter & Gamble\", \"Tyson Foods\"], analysis_type=\"Manager Title Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Î¼s, sys: 0 ns, total: 3 Î¼s\n",
      "Wall time: 5.48 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "findings = get_findings(data=sample_data, client_name=\"PepsiCo\", peer_list=[\"Archer Daniels Midland\", \"Nike\", \"Procter & Gamble\", \"Tyson Foods\"], analysis_type=\"Manager Title Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Î¼s, sys: 0 ns, total: 3 Î¼s\n",
      "Wall time: 5.48 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "findings = get_findings(data=sample_data, client_name=\"PepsiCo\", peer_list=[\"Archer Daniels Midland\", \"Nike\", \"Procter & Gamble\", \"Tyson Foods\"], analysis_type=\"Manager Title Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Î¼s, sys: 1e+03 ns, total: 3 Î¼s\n",
      "Wall time: 5.72 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "findings = get_findings(data=sample_data, client_name=\"PepsiCo\", peer_list=[\"Archer Daniels Midland\", \"Nike\", \"Procter & Gamble\", \"Tyson Foods\"], analysis_type=\"Manager Title Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PepsiCo has a manager title ratio of 37.9%, placing it slightly below '\n",
      " 'Procter & Gamble (41.5%) and Tyson Foods (41.6%), indicating a more '\n",
      " 'centralized management structure compared to its peers. In contrast, Archer '\n",
      " 'Daniels Midland and Nike have lower manager ratios at 29.7% and 26.0%, '\n",
      " \"respectively, showing that PepsiCo's managerial representation is \"\n",
      " 'competitive yet still below the top two in its peer group.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(findings[\"findings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PepsiCo has a notably high percentage of managers at 37.9%, which is '\n",
      " \"significantly above Archer Daniels Midland's 29.7% and Nike's 26.0%. \"\n",
      " 'However, PepsiCo is slightly lower than Procter & Gamble and Tyson Foods, '\n",
      " 'which have 41.5% and 41.6% managers respectively. This positions PepsiCo in '\n",
      " 'the upper mid-range for managerial representation compared to its peers.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(findings[\"findings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PepsiCo has a Manager representation of 37.9%, which is higher than Archer '\n",
      " 'Daniels Midland (29.7%) and Nike (26.0%), but slightly lower than Procter & '\n",
      " 'Gamble (41.5%) and Tyson Foods (41.6%). The overall manager ratio at PepsiCo '\n",
      " 'indicates a comparatively balanced management structure, positioning it '\n",
      " 'above some peers but below the two largest management ratios in the sample.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(findings[\"findings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PepsiCoâ€™s management layer comprises 37.9% of its workforce, ranking third '\n",
      " 'among its peers and sitting 3.6 points below the highest ratios at Procter & '\n",
      " 'Gamble (41.5%) and Tyson Foods (41.6%). This is substantially higher than '\n",
      " 'Archer Daniels Midland (29.7%) and Nike (26.0%), and about 3.2 points above '\n",
      " 'the peer average of 34.7%, indicating a stronger midâ€management presence '\n",
      " 'relative to most competitors.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(findings[\"findings\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
